# MED-VQA Pipeline for Diabetic Retinopathy<br/>
A model takes an image and text and outputs a natural-language answer. A typical Med-VQA architecture uses a joint-embedding pipeline: (CNN) encodes the image \, while a language encoder encodes the question text. These visual and textual features are fused and fed into an answer-generation module.<br/><br/>
