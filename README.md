# MED-VQA Pipeline for Diabetic Retinopathy<br/>
A model takes an image and text and outputs a natural-language answer. A typical Med-VQA architecture uses a joint-embedding pipeline: (CNN) encodes the image \, while a language encoder encodes the question text. These visual and textual features are fused and fed into an answer-generation module.<br/><br/>

---

## Resources<br/>

- [Fine-Tuning LLAVA â€” A Practical Guide](https://medium.com/@whyamit101/fine-tuning-llava-a-practical-guide-af606165a54c)<br/>

- [Fine-Tuning LLaVA-Med for Medical Imaging](https://medium.com/@sahaja2001vsj/fine-tuning-llava-med-for-medical-imaging-31e981a490af)
- [code](https://github.com/Veda0718/LLaVA-Med-Finetuning)<br/>
